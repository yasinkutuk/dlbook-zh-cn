\part{深度学习研究}
\label{part_research}

本书此部分介绍了当前深度学习研究社区中有关深度学习更深的愿景及更加高级的方法. \\

在本书前面的章节中，我们已经展示了如何去解决监督学习问题——在给定足够的映射样本的情况下，如何学习将一个向量映射到另一个向量. \\

不是所有我们想要解决的问题都是落在这个类别中的. 我们可能希望去产生新的样本，或者确定某些点是新的样本，或者处理遗漏值利用大量的无标记样本或者相关的任务的样本. 当前工业应用的最新发展的不足是我们的学习算法需要大量监督数据来达到足够的精确度. 在这个部分，我们会讨论一些降低对已有模型必要的标记数据量和能够应用在多个任务上的尝试性工作. 达成这些目标通常需要某种形式的无监督学习和半监督学习.\\

很多深度学习算法都是被设计成解决无监督学习问题的，但是相比深度学习在监督学习问题上已经取得的处处开花的成就，没有一个真正解决好这个问题. 本书此部分，我们给出已有的无监督学习方法和一些热门的有关如何在此领域取得进展的思考.\\

无监督学习的困难之处在于需要建模的随机变量的维度太高. 这会导致两个不同的挑战：统计学挑战和计算挑战. \emph{统计学挑战}说的是泛化：我们需要的能够区分类别的配置的数量会与有意义的维度的数量呈指数级增长，并且这数字很快超过我们可能拥有的样本数量（或者受到计算资源的限制）. 而 \emph{计算挑战}则是和高维分布相关，很多算法学习或者使用一个训练好的模型（特别是基于估计一个显式的概率函数）会引入与维度呈指数级增长的难解的计算量.\\

有了概率模型，计算挑战是来自进行难解推断或者就是规范化分布的需求.\\

\begin{enumerate}
\item \textbf{难解推断}：推断主要会放在第 \ref{ch:inference} 章讨论. 推断就是包含了在一个刻画了变量 $a$，$b$ 和 $c$ 联合分布的模型下，给定另外的变量 $b$，猜测某些变量 $a$ 可能的值的问题. 为了计算这种条件概率，我们需要对变量 $c$ 的可能值进行求和并且计算一个对 $a$ 和 $c$ 的值求和的规范化常量.
\item \textbf{难解规范化化常量（配分函数）}：这部分会在第 \ref{ch:partition} 章进行讨论. 规范化概率函数的常量在推断和学习中均会出现. 不幸的是，学习这样的模型通常需要计算配分函数关于模型参数的梯度. 这种计算通常和计算配分函数本身一样难解. 蒙特卡洛马尔科夫链（Monte Carlo Markov Chain，后简称 MCMC）方法（见第 \ref{ch:monte_carlo} 章）通常用来解决配分函数问题（计算本身或者其梯度）. 然而，MCMC 方法当模型分布的\gls*{mode}很多且分隔明显，特别是高维空间中时就遇到了麻烦（17.5 节）.
\end{enumerate}

一种处理这些难解计算问题的方法就是近似，很多近似方法在本书这个部分进行讨论. 另一个这里也会介绍的有趣方法就是通过设计来避免这些难解计算，这些方法就是非常有意义的. 近几年几种生成式模型已经被研究者提出来了，出发点就是良好的设计避免难解计算. 目前生成式模型的研究方法的几种变化在第 \ref{ch:generative_models} 章讨论. \\

第 \ref{part_research} 部分 对那些想要理解深度学习领域已有的观点的宽广，并且志在推进真正的人工智能领域的发展的研究者来说最为重要. 

